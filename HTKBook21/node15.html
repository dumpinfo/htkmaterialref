<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 3.0//EN">
<!--Converted with LaTeX2HTML 96.1 (Feb 5, 1996) by Nikos Drakos (nikos@cbl.leeds.ac.uk), CBLU, University of Leeds -->
<HTML>
<HEAD>
<TITLE>2.3.2 Training Tools</TITLE>
<META NAME="description" CONTENT="2.3.2 Training Tools">
<META NAME="keywords" CONTENT="HTKBook">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">
<LINK REL=STYLESHEET HREF="HTKBook.css">
</HEAD>
<BODY LANG="EN">
 <A NAME="tex2html1661" HREF="node16.html"><IMG WIDTH=37 HEIGHT=24 ALIGN=BOTTOM ALT="next" SRC="next_motif.gif"></A> <A NAME="tex2html1659" HREF="node13.html"><IMG WIDTH=26 HEIGHT=24 ALIGN=BOTTOM ALT="up" SRC="up_motif.gif"></A> <A NAME="tex2html1653" HREF="node14.html"><IMG WIDTH=63 HEIGHT=24 ALIGN=BOTTOM ALT="previous" SRC="previous_motif.gif"></A> <A NAME="tex2html1663" HREF="node1.html"><IMG WIDTH=65 HEIGHT=24 ALIGN=BOTTOM ALT="contents" SRC="contents_motif.gif"></A> <A NAME="tex2html1664" HREF="node263.html"><IMG WIDTH=43 HEIGHT=24 ALIGN=BOTTOM ALT="index" SRC="index_motif.gif"></A> <BR>
<B> Next:</B> <A NAME="tex2html1662" HREF="node16.html">2.3.3 Recognition Tools</A>
<B>Up:</B> <A NAME="tex2html1660" HREF="node13.html">2.3 The Toolkit</A>
<B> Previous:</B> <A NAME="tex2html1654" HREF="node14.html">2.3.1 Data Preparation Tools</A>
<BR> <P>
<H2><A NAME="SECTION02232000000000000000">2.3.2 Training Tools</A></H2>
<P>
The second step of system building is to<A NAME=1463>&#160;</A>
define the topology required for each HMM by writing a prototype definition.
HTK allows HMMs to be built with any desired topology.
HMM definitions can be stored externally as simple text files and
hence it is possible to edit them with any convenient text
editor. Alternatively, the standard HTK distribution includes
a number of example HMM prototypes and a script to generate
the most common topologies automatically.
With the exception of the transition
probabilities, all of the HMM parameters given in 
the prototype definition<A NAME=1464>&#160;</A>
are ignored.  The purpose of the prototype definition is only
to specify the overall characteristics and topology of the HMM.  The
actual parameters will be computed later by the training tools.  
Sensible values for
the transition probabilities must be given but the training
process is very insensitive to these.  An acceptable and  simple strategy
for choosing these probabilities is to make all of the transitions
out of any state equally likely.
<P>
<A NAME="fsysoview">&#160;</A> <IMG WIDTH=411 HEIGHT=430 ALIGN=BOTTOM ALT="tex2html_wrap19774" SRC="img107.gif"  > 
<P>
The actual training process takes place in stages and it is
illustrated in more detail in Fig.&nbsp;<A HREF="node15.html#ftsubword">2.3</A>.  
Firstly, an initial set of models must be created.  If there is
some speech data available for which the location of the sub-word (i.e. phone)
boundaries have been marked, then this can be used as <i>bootstrap data</i>.
In this case, the tools HINIT<A NAME=1654>&#160;</A> 
and HREST<A NAME=1655>&#160;</A>
provide <I>isolated word</I> style
training using the fully labelled bootstrap<A NAME=1475>&#160;</A> data.  Each of the required
HMMs is generated individually.  HINIT reads in all of the bootstrap
training data and <I>cuts out</I> all of the examples of the required
phone.  It then iteratively computes an initial set of parameter values
using a <I>segmental k-means</I> procedure<A NAME=1479>&#160;</A>.  On the first cycle, the training data
is uniformly segmented, each model state is matched with the
corresponding data segments and then means and variances are estimated.
If mixture Gaussian models are being trained, then a modified form
of k-means clustering is used.  On the second and successive cycles,
the uniform segmentation is replaced by Viterbi alignment.  
The initial parameter values computed by HINIT are then further re-estimated
by HREST.  Again, the fully labelled bootstrap data is used but this
time the segmental k-means procedure is replaced by the Baum-Welch re-estimation
procedure described in the previous chapter.  When no bootstrap data is
available, a so-called <i>flat start</i> can be used.  In this case all
of the phone models are initialised to be identical and have state means
and variances equal to the global speech mean and variance.  The tool
HCOMPV<A NAME=1656>&#160;</A> can be used for this.<A NAME=1485>&#160;</A>
<P>
<A NAME="ftsubword">&#160;</A> <IMG WIDTH=372 HEIGHT=499 ALIGN=BOTTOM ALT="tex2html_wrap19776" SRC="img108.gif"  > 
<P>
Once an initial set of models has been created, the tool HEREST
is used to perform <EM>embedded training</EM> using the 
entire<A NAME=1491>&#160;</A>
training set.  HEREST<A NAME=1657>&#160;</A> performs a single Baum-Welch
re-estimation of the whole set of HMM phone models simultaneously.  For each
training utterance, the corresponding phone models are concatenated and then
the forward-backward algorithm is used to accumulate the statistics of state
occupation, means, variances, etc., for each HMM in the sequence.  When
all of the training data has been processed, the accumulated statistics
are used to compute re-estimates of the HMM parameters.  HEREST is
the core HTK training tool.  It is designed to process large databases, it has
facilities for pruning<A NAME=1495>&#160;</A> to reduce computation and it can be run in parallel 
across a network of machines.
<P>
The philosophy of system construction in HTK  is that HMMs should be
<A NAME=1496>&#160;</A>
refined incrementally.  Thus, a typical progression is to start with a
simple set of single Gaussian context-independent phone models and then
iteratively refine them by expanding them to include context-dependency 
and use multiple mixture component Gaussian distributions.
The tool HHED<A NAME=1658>&#160;</A> is a 
HMM definition editor which will clone models<A NAME=1499>&#160;</A>
into context-dependent sets, apply a variety of parameter tyings
and increment the number of mixture components in specified distributions.
The usual process is to modify a set of HMMs in stages using HHED
and then 
re-estimate the parameters of the modified set using HEREST
after each stage.
<P>
The single biggest problem in building context-dependent 
HMM systems is always data
insufficiency.  The more complex the model set, the more data is 
needed to make robust estimates of its parameters, and since data is
usually limited, a balance must be struck between complexity and 
the available data.
For continuous density systems, this balance is achieved by 
tying parameters together as mentioned above.  Parameter tying
allows data to be pooled so that the shared parameters can be robustly
estimated.  In addition to continuous density systems, HTK also supports
fully tied mixture systems and discrete probability systems.  In these
cases, the data insufficiency problem is usually addressed by smoothing
the distrutions and the 
tool HSMOOTH<A NAME=1659>&#160;</A> is used for this.
<P>
<HR><A NAME="tex2html1661" HREF="node16.html"><IMG WIDTH=37 HEIGHT=24 ALIGN=BOTTOM ALT="next" SRC="next_motif.gif"></A> <A NAME="tex2html1659" HREF="node13.html"><IMG WIDTH=26 HEIGHT=24 ALIGN=BOTTOM ALT="up" SRC="up_motif.gif"></A> <A NAME="tex2html1653" HREF="node14.html"><IMG WIDTH=63 HEIGHT=24 ALIGN=BOTTOM ALT="previous" SRC="previous_motif.gif"></A> <A NAME="tex2html1663" HREF="node1.html"><IMG WIDTH=65 HEIGHT=24 ALIGN=BOTTOM ALT="contents" SRC="contents_motif.gif"></A> <A NAME="tex2html1664" HREF="node263.html"><IMG WIDTH=43 HEIGHT=24 ALIGN=BOTTOM ALT="index" SRC="index_motif.gif"></A> <BR>
<B> Next:</B> <A NAME="tex2html1662" HREF="node16.html">2.3.3 Recognition Tools</A>
<B>Up:</B> <A NAME="tex2html1660" HREF="node13.html">2.3 The Toolkit</A>
<B> Previous:</B> <A NAME="tex2html1654" HREF="node14.html">2.3.1 Data Preparation Tools</A>
<P><ADDRESS>
ECRL HTK_V2.1: email support@entropic.com
</ADDRESS>
</BODY>
</HTML>
