<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 3.0//EN">
<!--Converted with LaTeX2HTML 96.1 (Feb 5, 1996) by Nikos Drakos (nikos@cbl.leeds.ac.uk), CBLU, University of Leeds -->
<HTML>
<HEAD>
<TITLE>12.1 Decoder Operation</TITLE>
<META NAME="description" CONTENT="12.1 Decoder Operation">
<META NAME="keywords" CONTENT="HTKBook">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">
<LINK REL=STYLESHEET HREF="HTKBook.css">
</HEAD>
<BODY LANG="EN">
 <A NAME="tex2html3362" HREF="node140.html"><IMG WIDTH=37 HEIGHT=24 ALIGN=BOTTOM ALT="next" SRC="next_motif.gif"></A> <A NAME="tex2html3360" HREF="node138.html"><IMG WIDTH=26 HEIGHT=24 ALIGN=BOTTOM ALT="up" SRC="up_motif.gif"></A> <A NAME="tex2html3354" HREF="node138.html"><IMG WIDTH=63 HEIGHT=24 ALIGN=BOTTOM ALT="previous" SRC="previous_motif.gif"></A> <A NAME="tex2html3364" HREF="node1.html"><IMG WIDTH=65 HEIGHT=24 ALIGN=BOTTOM ALT="contents" SRC="contents_motif.gif"></A> <A NAME="tex2html3365" HREF="node263.html"><IMG WIDTH=43 HEIGHT=24 ALIGN=BOTTOM ALT="index" SRC="index_motif.gif"></A> <BR>
<B> Next:</B> <A NAME="tex2html3363" HREF="node140.html">12.2 Decoder Organisation</A>
<B>Up:</B> <A NAME="tex2html3361" HREF="node138.html">12 Decoding</A>
<B> Previous:</B> <A NAME="tex2html3355" HREF="node138.html">12 Decoding</A>
<BR> <P>
<H1><A NAME="SECTION03910000000000000000">12.1 Decoder Operation</A></H1>
<A NAME="sdecop">&#160;</A>
<P>
<A NAME=11131>&#160;</A>
As described in Chapter&nbsp;<A HREF="node128.html#cnetdict">11</A> and illustrated by
Fig.&nbsp;<A HREF="node129.html#frecsys">11.1</A>, decoding in HTK is controlled by a recognition
network compiled from a word-level network, a dictionary and a set of
HMMs.  The recognition network consists of a set of nodes connected
by arcs.  Each node is either a HMM model instance or a word-end.
Each model node is itself a network consisting of states connected by
arcs.  Thus, once fully compiled, a recognition 
network<A NAME=11134>&#160;</A> ultimately
consists of HMM states connected by transitions.  However, it can be
viewed at three different levels: word, model and state.
Fig.&nbsp;<A HREF="node139.html#frecnetlev">12.1</A> illustrates this hierarchy.
<P>

<A NAME="frecnetlev">&#160;</A> <IMG WIDTH=293 HEIGHT=299 ALIGN=BOTTOM ALT="tex2html_wrap22086" SRC="img465.gif"  > 
<P>
For an unknown
input utterance with <I>T</I> frames, every path from the start node to the
exit node of the network which passes through exactly <I>T</I> emitting HMM
states is a potential recognition 
hypothesis<A NAME=11472>&#160;</A>.
Each of these paths has a log probability which is computed by summing
the log probability of each individual transition in the path and the
log probability of each emitting state generating the corresponding
observation.  Within-HMM transitions are determined from the HMM
parameters, between-model transitions are constant and word-end
transitions are determined by the language model likelihoods attached
to the word level networks.
<P>
The job of the decoder is to find those paths through the network
which have the highest log probability.  These paths
are found using a <i>Token Passing</i> algorithm.  A token represents
a partial path through the network extending from time 0 through to time <I>t</I>.
At time 0, a token is placed in every possible start node.  <A NAME=11474>&#160;</A>
<P>

<P>
Each time step,
tokens are propagated along connecting transitions stopping whenever they
reach an emitting HMM state.  When there are multiple exits from a node,
the token is copied so that all possible paths are explored in parallel.
As the token passes across transitions and through nodes, its log probability
is incremented by the corresponding transition and emission probabilities.
A network node can hold at most <I>N</I> tokens.  Hence, at the end of each time step,
all but the <I>N</I> best tokens in any node are discarded.
<P>
As each token passes through the network it must maintain a history
recording its route.  The amount of detail in this history<A NAME=11143>&#160;</A> depends
on the required recognition output.  Normally, only word sequences
are wanted and hence, only transitions out of word-end nodes<A NAME=11144>&#160;</A> need
be recorded.  However, for some purposes, it is useful to know the
actual model sequence and the time of each model to model transition.
Sometimes a description of each path down to the state level
is required.  All of this information, whatever level of detail is
required, can conveniently be represented using a lattice structure.
<P>
Of course, the number of tokens allowed per node and the amount of
history information requested will have a significant impact on
the time and memory needed to compute the lattices.  The most
efficient configuration is <I>N</I>=1 combined with just 
word level history information and this is sufficient
for most purposes.
<P>
A large network will have many nodes and one way to make a significant
reduction in the computation needed is to only propagate tokens which
have some chance of being amongst the eventual winners.  This process
is called <i>pruning</i>.  It is implemented at each time step by
keeping a record of the best token overall and de-activating all
tokens whose log probabilities fall more than a <i>beam-width</i>
below the best.  For efficiency reasons, it is best to implement primary
pruning<A NAME=11147>&#160;</A> at the model rather than the state level.  Thus, models
are deactivated when they have no tokens in any state within the beam and
they are reactivated whenever active tokens are propagated into them.
State-level pruning is also implemented by replacing any token by a 
null (zero probability) token if it falls outside of the beam.
If the pruning beam-width<A NAME=11148>&#160;</A> is set too small then the most likely
path might be pruned before its token reaches the end of the utterance.
This results in a <i>search error</i>.  Setting the beam-width is
thus a compromise between speed and avoiding search errors.
<P>
When using word loops with bigram probabilities, tokens emitted from
word-end nodes will have a language model probability added to them
before entering the following word.  Since the range of language
model probabilities is relatively small, a narrower beam can be
applied to word-end nodes without incurring additional 
search errors<A NAME=11150>&#160;</A>.
This beam is calculated relative to the best word-end token and
it is called a <i>word-end beam</i>.  In the case, of a recognition
network with an arbitrary topology, word-end pruning may still be
beneficial but this can only be justified empirically.
<P>
Finally, a third type of pruning control is provided.  An upper-bound
on the allowed use of compute resource can be applied by setting
an upper-limit on the number of models in the network which can
be active simultaneously.  When this limit is reached, the pruning
beam-width is reduced in order to prevent it being exceeded.
<P>
<HR><A NAME="tex2html3362" HREF="node140.html"><IMG WIDTH=37 HEIGHT=24 ALIGN=BOTTOM ALT="next" SRC="next_motif.gif"></A> <A NAME="tex2html3360" HREF="node138.html"><IMG WIDTH=26 HEIGHT=24 ALIGN=BOTTOM ALT="up" SRC="up_motif.gif"></A> <A NAME="tex2html3354" HREF="node138.html"><IMG WIDTH=63 HEIGHT=24 ALIGN=BOTTOM ALT="previous" SRC="previous_motif.gif"></A> <A NAME="tex2html3364" HREF="node1.html"><IMG WIDTH=65 HEIGHT=24 ALIGN=BOTTOM ALT="contents" SRC="contents_motif.gif"></A> <A NAME="tex2html3365" HREF="node263.html"><IMG WIDTH=43 HEIGHT=24 ALIGN=BOTTOM ALT="index" SRC="index_motif.gif"></A> <BR>
<B> Next:</B> <A NAME="tex2html3363" HREF="node140.html">12.2 Decoder Organisation</A>
<B>Up:</B> <A NAME="tex2html3361" HREF="node138.html">12 Decoding</A>
<B> Previous:</B> <A NAME="tex2html3355" HREF="node138.html">12 Decoding</A>
<P><ADDRESS>
ECRL HTK_V2.1: email support@entropic.com
</ADDRESS>
</BODY>
</HTML>
