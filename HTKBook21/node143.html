<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 3.0//EN">
<!--Converted with LaTeX2HTML 96.1 (Feb 5, 1996) by Nikos Drakos (nikos@cbl.leeds.ac.uk), CBLU, University of Leeds -->
<HTML>
<HEAD>
<TITLE>12.5 Generating Forced Alignments</TITLE>
<META NAME="description" CONTENT="12.5 Generating Forced Alignments">
<META NAME="keywords" CONTENT="HTKBook">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">
<LINK REL=STYLESHEET HREF="HTKBook.css">
</HEAD>
<BODY LANG="EN">
 <A NAME="tex2html3410" HREF="node144.html"><IMG WIDTH=37 HEIGHT=24 ALIGN=BOTTOM ALT="next" SRC="next_motif.gif"></A> <A NAME="tex2html3408" HREF="node138.html"><IMG WIDTH=26 HEIGHT=24 ALIGN=BOTTOM ALT="up" SRC="up_motif.gif"></A> <A NAME="tex2html3402" HREF="node142.html"><IMG WIDTH=63 HEIGHT=24 ALIGN=BOTTOM ALT="previous" SRC="previous_motif.gif"></A> <A NAME="tex2html3412" HREF="node1.html"><IMG WIDTH=65 HEIGHT=24 ALIGN=BOTTOM ALT="contents" SRC="contents_motif.gif"></A> <A NAME="tex2html3413" HREF="node263.html"><IMG WIDTH=43 HEIGHT=24 ALIGN=BOTTOM ALT="index" SRC="index_motif.gif"></A> <BR>
<B> Next:</B> <A NAME="tex2html3411" HREF="node144.html">12.6 Recognition using Direct Audio Input</A>
<B>Up:</B> <A NAME="tex2html3409" HREF="node138.html">12 Decoding</A>
<B> Previous:</B> <A NAME="tex2html3403" HREF="node142.html">12.4 Evaluating Recognition Results</A>
<BR> <P>
<H1><A NAME="SECTION03950000000000000000">12.5 Generating Forced Alignments</A></H1>
<A NAME="sfalign">&#160;</A>
<P>
<A NAME=11303>&#160;</A>
<P>

<A NAME="fhvalign">&#160;</A> <IMG WIDTH=227 HEIGHT=156 ALIGN=BOTTOM ALT="tex2html_wrap22120" SRC="img473.gif"  > 
<P>
HVITE can be made to compute forced alignments by not 
specifying a network with the <tt>-w</tt> option but by specifying
the <tt>-a</tt> option instead.  In this mode, HVITE 
computes a new network for each input utterance using the word
level transcriptions and a dictionary.  By default, the output
transcription will just contain the words and their boundaries.
One of the main uses of forced alignement<A NAME=11594>&#160;</A>, 
however, is to 
determine the actual pronunciations used in the utterances
used to train the HMM system in this case, the <tt>-m</tt>
option can be used to generate model level output transcriptions.
<P>

<P>
This type of forced alignment is usually part of a <i>bootstrap</i>
process, initially models are trained on the basis of one fixed
pronunciation per <A NAME=11414>&#160;</A>
<A NAME=11415>&#160;</A>word<A NAME="tex2html900" HREF="footnode.html#11416"><IMG  ALIGN=BOTTOM ALT="gif" SRC="foot_motif.gif"></A>.  
Then HVITE is used in forced alignment mode
to select the best matching pronuciations.  The new phone level
transcriptions can then be used to retrain the HMMs.  Since training
data may have leading and trailing silence, it is usually
necessary to insert a silence model at the start and end of the
recognition network.  The  <tt>-b</tt> option can be used to do this.
<P>
As an illustration, executing
<PRE> HVite -a -b sil -m -o SWT -I words.mlf -H hmmset dict hmmlist file.mfc</PRE>
would result in the following sequence of events (see Fig.&nbsp;<A HREF="node143.html#fhvalign">12.3</A>).
The input file name <tt>file.mfc</tt> would have its extension replaced by
<tt>lab</tt> and then a label file of this name would be searched for.
In this case, the MLF file <tt>words.mlf</tt> has been loaded. 
Assuming that this file contains a word level transcription called
<tt>file.lab</tt>, this transcription along with the dictionary <tt>dict</tt>
will be used to construct a network equivalent to <tt>file.lab</tt>
but with alternative pronunciations included in parallel.  Since <tt>-b</tt>
option has been set, the specified <tt>sil</tt> model will be inserted
at the start and end of the network.  The decoder then finds the best
matching path through the network and constructs a lattice which
includes model alignment information.  Finally, the lattice is converted
to a transcription and output to the label file <tt>file.rec</tt>.
As for testing on a database, alignments will normally be computed on
a large number of input files so in practice the input files would be listed
in a <tt>.scp</tt> file and the output transcriptions would be written
to an MLF using the <tt>-i</tt> option.
<P>
When the <tt>-m</tt> option is used, the transcriptions output by HVITE 
would by default contain both the model level and 
word level transcriptions
<A NAME=11335>&#160;</A>.
<A NAME=11336>&#160;</A>
<A NAME=11337>&#160;</A>
For example, a typical fragment of the output might be
<PRE>    7500000  8700000 f  -1081.604736 FOUR 30.000000
    8700000  9800000 ao  -903.821350
    9800000 10400000 r   -665.931641
   10400000 10400000 sp    -0.103585
   10400000 11700000 s  -1266.470093 SEVEN 22.860001
   11700000 12500000 eh  -765.568237
   12500000 13000000 v   -476.323334
   13000000 14400000 n  -1285.369629
   14400000 14400000 sp    -0.103585</PRE>
Here the score alongside each model name is the acoustic score for that segment.
The score alongside the word is just the language model score.
<P>
Although the above information can be useful for some purposes, for example
in bootstrap training, only the model names are required.
The formatting option <tt>-o SWT</tt> in the above suppresses all output
except the model names.<A NAME=11339>&#160;</A>
<P>
<HR><A NAME="tex2html3410" HREF="node144.html"><IMG WIDTH=37 HEIGHT=24 ALIGN=BOTTOM ALT="next" SRC="next_motif.gif"></A> <A NAME="tex2html3408" HREF="node138.html"><IMG WIDTH=26 HEIGHT=24 ALIGN=BOTTOM ALT="up" SRC="up_motif.gif"></A> <A NAME="tex2html3402" HREF="node142.html"><IMG WIDTH=63 HEIGHT=24 ALIGN=BOTTOM ALT="previous" SRC="previous_motif.gif"></A> <A NAME="tex2html3412" HREF="node1.html"><IMG WIDTH=65 HEIGHT=24 ALIGN=BOTTOM ALT="contents" SRC="contents_motif.gif"></A> <A NAME="tex2html3413" HREF="node263.html"><IMG WIDTH=43 HEIGHT=24 ALIGN=BOTTOM ALT="index" SRC="index_motif.gif"></A> <BR>
<B> Next:</B> <A NAME="tex2html3411" HREF="node144.html">12.6 Recognition using Direct Audio Input</A>
<B>Up:</B> <A NAME="tex2html3409" HREF="node138.html">12 Decoding</A>
<B> Previous:</B> <A NAME="tex2html3403" HREF="node142.html">12.4 Evaluating Recognition Results</A>
<P><ADDRESS>
ECRL HTK_V2.1: email support@entropic.com
</ADDRESS>
</BODY>
</HTML>
